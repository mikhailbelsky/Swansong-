{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "339f6b72-aa28-4bae-8966-3e64de808ce6",
   "metadata": {},
   "source": [
    "# Logistic Regression Model: Customer Churn Risk Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff07114e-30fe-4b36-9439-a2f4d9090183",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Import libraries, read in the data, introduce some useful functions\n",
    "\n",
    "# import the big 4\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Import performance metrics\n",
    "from sklearn import metrics\n",
    "from time import time\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve\n",
    "from sklearn.metrics import precision_recall_curve, f1_score\n",
    "\n",
    "## Import Logistic Regression from sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Import ensemble models\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Accurancy, precision, recall function\n",
    "\n",
    "def apr(y_pred, y_real):\n",
    "    accuracy = metrics.accuracy_score(y_real, y_pred)\n",
    "    precision = metrics.precision_score(y_real, y_pred)\n",
    "    recall = metrics.recall_score(y_real, y_pred)\n",
    "    f1 = metrics.f1_score(y_real, y_pred)\n",
    "    \n",
    "    print(f\"Accuracy:{accuracy}\")\n",
    "    print(f\"Precision:{precision}\")\n",
    "    print(f\"Recall:{recall}\")\n",
    "    print(f\"F1:{f1}\")\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "## Confusion matrix function\n",
    "\n",
    "def produce_confusion(positive_label, negative_label, cut_off, df, y_pred_name, y_real_name):\n",
    "    \n",
    "    #Set pred to 0 or 1 depending on whether it's higher than the cut_off point.\n",
    "    \n",
    "    if cut_off != 'binary':      \n",
    "        df['pred_binary'] = np.where(df[y_pred_name] > cut_off , 1, 0)\n",
    "    else: \n",
    "        df['pred_binary'] = df[y_pred_name]\n",
    "    \n",
    "    #Build the CM\n",
    "    cm = confusion_matrix(df[y_real_name], df['pred_binary'])  \n",
    "    \n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax=ax, fmt='g'); \n",
    "\n",
    "    # labels, title, ticks\n",
    "    ax.set_xlabel('Predicted labels');ax.set_ylabel('Real labels'); \n",
    "    ax.set_title('Confusion Matrix'); \n",
    "    ax.xaxis.set_ticklabels([negative_label, positive_label])\n",
    "    ax.yaxis.set_ticklabels([negative_label, positive_label]);\n",
    "\n",
    "    print('Test accuracy = ', accuracy_score(df[y_real_name], df['pred_binary']))\n",
    "\n",
    "    return accuracy_score(df[y_real_name], df['pred_binary'])\n",
    "\n",
    "df = pd.read_excel(\"1 - Project Data.xlsx\")\n",
    "\n",
    "dropped_customers = df[df['Total Charges'].str.strip() == '']\n",
    "\n",
    "# Mapping columns to get binary results\n",
    "def columns_binary(x):\n",
    "    if x == 'Yes':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7875d575-1da4-41e6-9775-2b9cc426d5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feb3316-c946-457c-a4ef-87662ed3e83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Feature Engineer & Train Test Split (data leakage avoided: no null-handling aggregations)\n",
    "def feature_eng(df):\n",
    "\n",
    "    # Drop rows where 'Total Charges' is missing or empty\n",
    "    # We have 11 rows with empty Total Charges which we decided to drop rather than replace\n",
    "    df['Total Charges'] = pd.to_numeric(df['Total Charges'], errors='coerce')  # Ensure 'Total Charges' is numeric\n",
    "    df = df.dropna(subset=['Total Charges'])  # Drop rows with missing 'Total Charges'\n",
    "\n",
    "    \n",
    "    # Drop non-useful columns\n",
    "    df = df.drop(columns = ['CustomerID', \n",
    "                            'Count', \n",
    "                            'Country',\n",
    "                            'Zip Code',\n",
    "                            'State', \n",
    "                            'Lat Long', \n",
    "                            'Latitude', \n",
    "                            'Longitude', \n",
    "                            'Churn Label',\n",
    "                            'Churn Reason'],\n",
    "                            axis = 1) # We already have Churn Value which is already a numerical \n",
    "\n",
    "    \n",
    "    # Change yes/no(including: No internet/Phone services) to binary\n",
    "    df['Senior Citizen'] = df['Senior Citizen'].apply(columns_binary)\n",
    "    df['Partner'] = df['Partner'].apply(columns_binary)\n",
    "    df['Dependents'] = df['Dependents'].apply(columns_binary)\n",
    "    df['Phone Service'] = df['Phone Service'].apply(columns_binary)\n",
    "    df['Paperless Billing'] = df['Paperless Billing'].apply(columns_binary)\n",
    "\n",
    "    # Changing columns with only two results to numericals\n",
    "    df['Gender'] = df['Gender'].map({'Male':0, 'Female':1})\n",
    "\n",
    "    # Changing columns with 3 results to numericals\n",
    "    df['Multiple Lines'] = df['Multiple Lines'].map({'No':0, 'Yes':1, 'No phone service':2})\n",
    "    df['Online Security'] = df['Online Security'].map({'No':0, 'Yes':1, 'No internet service':2})\n",
    "    df['Device Protection'] = df['Device Protection'].map({'No':0, 'Yes':1, 'No internet service':2})\n",
    "    df['Online Backup'] = df['Online Backup'].map({'No':0, 'Yes':1, 'No internet service':2})\n",
    "    df['Tech Support'] = df['Tech Support'].map({'No':0, 'Yes':1, 'No internet service':2})\n",
    "    df['Streaming TV'] = df['Streaming TV'].map({'No':0, 'Yes':1, 'No internet service':2})\n",
    "    df['Streaming Movies'] = df['Streaming Movies'].map({'No':0, 'Yes':1, 'No internet service':2})\n",
    "\n",
    "    \n",
    "    # OHE columns\n",
    "    df = pd.get_dummies(df, columns = ['Internet Service'], drop_first = True, prefix = 'Internet Service', dtype=int)\n",
    "    df = pd.get_dummies(df, columns = ['Contract'], drop_first = True, prefix = 'Contract', dtype=int)\n",
    "    df = pd.get_dummies(df, columns = ['Payment Method'], drop_first = True, prefix = 'Payment Method', dtype=int)\n",
    "\n",
    "    # Map cities to unique integers\n",
    "    city_mapping = {city: idx for idx, city in enumerate(df['City'].unique())}\n",
    "    df['City'] = df['City'].map(city_mapping)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "featured_df = feature_eng(df)\n",
    "\n",
    "\n",
    "# Define feature columns for our predictions\n",
    "feature_cols = ['City', 'Senior Citizen', 'Partner', 'Dependents',\n",
    "               'Tenure Months', 'Online Security',\n",
    "               'Online Backup', 'Device Protection', 'Tech Support', 'Streaming TV',\n",
    "               'Streaming Movies', 'Paperless Billing', 'Monthly Charges',\n",
    "               'Total Charges', 'Internet Service_Fiber optic',\n",
    "               'Internet Service_No', 'Contract_One year', 'Contract_Two year',\n",
    "               'Payment Method_Credit card (automatic)',\n",
    "               'Payment Method_Electronic check', 'Payment Method_Mailed check'\n",
    "               ]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(featured_df[feature_cols],\n",
    "                                                   featured_df['Churn Value'],\n",
    "                                                   test_size = 0.3,\n",
    "                                                   random_state = 99,\n",
    "                                                   stratify = featured_df['Churn Value'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6c3e2d-84ed-4b4b-a213-23bb2546d5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Scale some features\n",
    "\n",
    "columns_to_scale = ['Monthly Charges', 'Total Charges']\n",
    "scaler = StandardScaler()\n",
    "X_train[columns_to_scale] = scaler.fit_transform(X_train[columns_to_scale])\n",
    "X_test[columns_to_scale] = scaler.transform(X_test[columns_to_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb59824b-0ee0-4fab-ab0d-d1bf9c549fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Fit the model on training set \n",
    "\n",
    "lr = LogisticRegression(class_weight='balanced', max_iter=5000, random_state=99)\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e673bf-3cf6-47ff-84e1-d275967b8c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Find optimal decision boundary for F1 Score (> 0.7 = GOOD)\n",
    "\n",
    "# Get predicted probabilities\n",
    "y_pred_proba = lr.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
    "\n",
    "# Precision-Recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
    "\n",
    "# Compute F1 scores for all thresholds\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# Find the threshold that gives the highest F1-score\n",
    "optimal_idx = f1_scores.argmax()\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "print(f\"Optimal Threshold: {optimal_threshold}\")\n",
    "print(f\"Precision: {precision[optimal_idx]}, Recall: {recall[optimal_idx]}, F1: {f1_scores[optimal_idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fcda49-6b95-4d6c-93a3-231d21cb5ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Find optimal decision boundary for ROC-AUC (0.7 to 0.8 = GOOD)\n",
    "y_pred_proba = lr.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "# Find the optimal threshold (e.g., Youden's J statistic = TPR - FPR)\n",
    "optimal_idx = (tpr - fpr).argmax()\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "print(f\"Optimal Threshold: {optimal_threshold}\")\n",
    "print(f\"True Positive Rate: {tpr[optimal_idx]}, False Positive Rate: {fpr[optimal_idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77482bf-3d87-4fa8-8c63-545a359d40e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 7: Predict probabilities, store each binary prediction in a column corresponding to its class\n",
    "X_train[['active', 'churned']] = lr.predict_proba(X_train)\n",
    "X_train['y_pred'] = np.where(X_train['churned']>.6, 1, 0) # decision boundary reflecting optimal threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af87c7b4-607d-484a-93c8-36f097af8315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Evaluate model on test \n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Predictions\n",
    "y_pred = lr.predict(X_test)\n",
    "y_pred_proba = lr.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Print metrics\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09d5fd0-d12c-45bc-ad4d-81851026c4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(actual, predicted):\n",
    "    print(\"The confusion matrix for your predictions is:\")\n",
    "    print(metrics.confusion_matrix(actual, predicted), \"\\n\")\n",
    "    print(f'The accuracy of your model is: {metrics.accuracy_score(actual, predicted)}')\n",
    "    print(f'The recall of your model is: {metrics.recall_score(actual, predicted)}')\n",
    "    print(f'The precision of your model is: {metrics.precision_score(actual, predicted)}')\n",
    "    print(f'The F1-score of your model is: {metrics.f1_score(actual, predicted)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbdeaf7-aa27-4507-a63b-7e72773f222c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 9: Check Performance Metrics\n",
    "get_results(y_train, X_train['y_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12df4e87-7fc4-4191-9c13-3f60c5e7e52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Create table of customer ids & their churn probabilities (test)\n",
    "\n",
    "# Merge y_test (actual churn status) into X_test for clarity\n",
    "X_test['actual_churn'] = y_test.values  # Add actual churn labels\n",
    "\n",
    "# Predict probabilities for the feature set\n",
    "X_test_clean = X_test[feature_cols] #extract feature columns from x_test\n",
    "y_pred_proba = lr.predict_proba(X_test_clean)[:, 1] #predict probabilities for test\n",
    "\n",
    "# Add churn probabilities back to the original X_test\n",
    "X_test['churn_probability'] = y_pred_proba\n",
    "\n",
    "# Filter for actives (Churn Value = 0)\n",
    "final_churner = X_test[X_test['actual_churn'] == 0]\n",
    "\n",
    "final_churner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f1c8c2-b3aa-4dae-b758-a1628551c23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: Create table of customer ids & their churn probabilities (train)\n",
    "\n",
    "X_train['actual_churn'] = y_train.values  # Add actual churn labels\n",
    "\n",
    "# Predict probabilities for the feature set\n",
    "X_train_clean = X_train[feature_cols] #extract feature columns from x_test\n",
    "y_pred_prob = lr.predict_proba(X_train_clean)[:, 1] #predict probabilities for \n",
    "\n",
    "# Add churn probabilities back to the original X_train\n",
    "X_train['churn_probability'] = y_pred_prob\n",
    "\n",
    "# Filter for actives (Churn Value = 0)\n",
    "total_churners = X_train[X_train['actual_churn'] == 0]\n",
    "\n",
    "# Sort by predicted churn probability\n",
    "total_churners.sort_values(by='churn_probability', ascending=False)\n",
    "\n",
    "# Display the top 20 churners\n",
    "total_churners.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f40950-a1c4-4f76-b870-c7554d1d62fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: Concatenate the two dataframes\n",
    "final_table = pd.concat([total_churners, final_churner], axis=0)\n",
    "final_table.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45370eab-8b82-4503-bc78-48e0d4bb540f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_churners.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c12c314-aaca-406f-ac57-f7e6f63eeb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85d8ce7-11d8-470f-b25b-6823d4fa85fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12: Add Customer ID back in, reset index\n",
    "\n",
    "og_df = pd.read_excel(\"1 - Project Data.xlsx\")\n",
    "\n",
    "# Reset index in both DataFrames to align them properly\n",
    "final_table = final_table.reset_index()\n",
    "og_df = og_df.reset_index()\n",
    "\n",
    "# Merge CustomerID back into final table using the index\n",
    "total_churners_with_id = final_table.merge(og_df[['CustomerID']], \n",
    "                                      left_index=True, \n",
    "                                      right_index=True)\n",
    "\n",
    "# Select only CustomerID and churn_probability for a clean output\n",
    "total_churners_df = total_churners_with_id[['CustomerID', 'churn_probability']]\n",
    "\n",
    "# Display the clean DataFrame\n",
    "total_churners_df.head(10) # Show the top 10 for verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77746bf2-2b15-480e-8dc8-aa4a91a15a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final: export tables to csv/excel \n",
    "total_churners_df.to_csv(\"Customer Churn Risk 2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c04d61-7547-4893-82f4-24e1bb6015f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_churners_df.to_excel(\"Customer Churn Risk 2.xlsx\", index=False, engine=\"openpyxl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49243d2a-c9e9-4533-9aa9-5017d94ad430",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
